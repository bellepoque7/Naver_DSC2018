data2$description = gsub('\n|\t|<.*?>|&quot;',' ',data2$description)
data2$description = gsub('[^가-힣a-zA-Z]',' ',data2$description)
data2$description = gsub(' +',' ',data2$description)
nouns=KoNLP::extractNoun(final_data2$description)
data_unlist = unlist(nouns)
filter_data = Filter(function(x){nchar(x)>=2}, data_unlist)
filter_data_table = table(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
source('~/GitHub/SNU_BDI_-JJJ-R/(과제) Web.R', encoding = 'UTF-8')
install.packages(c("KoNLP", "wordcloud"))
install.packages("rJava")
install.packages("tm")
install.packages("rJava")
install.packages("tm")
rm(list = ls()); gc(reset = T)
if(!require(rvest)){install.packages('rvest') ; library(rvest)}
if(!require(httr)){install.packages('httr') ; library(httr)}
if(!require(KoNLP)){install.packages('KoNLP') ; library(KoNLP)} # 세종사전
if(!require(wordcloud)){install.packages('wordcloud') ; library(wordcloud)} # RColorBrewer()함수 제공
if(!require(yaml)){install.packages('yaml') ; library(yaml)}
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
library(tm) #영문 텍스트 마이닝
client_id = 'AIhq1yM8BsaR1QA8S_1S';
client_secret = 'KcwLvPLKUx';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
#블로그출처
# ---------------------
query = '곱창'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
# ---------------------
end_num = 1000
display_num = 100
start_point = seq(1,end_num,display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',
query,'&display=',display_num,'&start=',
start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
# ---------------------
i = 1
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat_orgin = data.frame(final_dat, stringsAsFactors = F)
data2 = final_dat_orgin  # 블로그 주소 , 날짜
data2$description = gsub('\n|\t|<.*?>|&quot;',' ',data2$description)
data2$description = gsub('[^가-힣a-zA-Z]',' ',data2$description)
data2$description = gsub(' +',' ',data2$description)
nouns=KoNLP::extractNoun(final_data2$description)
nouns=KoNLP::extractNoun(data2$description)
data_unlist = unlist(nouns)
filter_data = Filter(function(x){nchar(x)>=2}, data_unlist)
filter_data_table = table(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
str(filter_data)
head(filter_data)
View(filter_data)
client_id = 'AIhq1yM8BsaR1QA8S_1S';
client_secret = 'KcwLvPLKUx';
header = httr::add_headers(
'X-Naver-Client-Id' = client_id,
'X-Naver-Client-Secret' = client_secret)
#블로그출처
# ---------------------
query = '곱창'
# encoding 변화
query = iconv(query, to = 'UTF-8', toRaw = T)
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query = toupper(query)
# ---------------------
end_num = 1000
display_num = 100
start_point = seq(1,end_num,display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',
query,'&display=',display_num,'&start=',
start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
# ---------------------
i = 1
final_dat = NULL
for(i in 1:length(start_point))
{
# request xml format
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
#option header
url_body = read_xml(GET(url, header), encoding = "UTF-8")
title = url_body %>% xml_nodes('item title') %>% xml_text()
bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
link = url_body %>% xml_nodes('item link') %>% xml_text()
description = url_body %>% xml_nodes('item description') %>% html_text()
temp_dat = cbind(title, bloggername, postdate, link, description)
final_dat = rbind(final_dat, temp_dat)
cat(i, '\n')
}
final_dat_orgin = data.frame(final_dat, stringsAsFactors = F)
data2 = final_dat_orgin  # 블로그 주소 , 날짜
data2$description = gsub('\n|\t|<.*?>|&quot;',' ',data2$description)
data2$description = gsub('[^가-힣a-zA-Z]',' ',data2$description)
data2$description = gsub(' +',' ',data2$description)
data2$description = gsub('곱창',' ',data2$description)
data2$description = gsub('으로',' ',data2$description)
data2$description = gsub('진짜',' ',data2$description)
data2$description = gsub('오랜만',' ',data2$description)
# 여기서 '곱창','으로' '마마(?)' ,'진짜'오랜만 을
# Gsub함수로 지우기
nouns=KoNLP::extractNoun(data2$description)
data_unlist = unlist(nouns)
filter_data = Filter(function(x){nchar(x)>=2}, data_unlist)
filter_data_table = table(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
View(filter_data)
?sub
data2$description = gsub('\n|\t|<.*?>|&quot;',' ',data2$description)
data2$description = gsub('[^가-힣a-zA-Z]',' ',data2$description)
data2$description = gsub(' +',' ',data2$description)
data2$description = gsub('곱창',' ',data2$description)
data2$description = gsub('으로',' ',data2$description)
data2$description = gsub('진짜',' ',data2$description)
data2$description = gsub('오랜만',' ',data2$description)
data2$description = gsub('근번ㅊ',' ',data2$description)
data2$description = gsub('이번',' ',data2$description)
data2$description = gsub('얼마',' ',data2$description)
str(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
View(filter_data)
wordcloud(mydata, filter_data_table)
mydata <- names(filter_data_table)
wordcloud(mydata, filter_data_table)
?wordcloud
data_number< -sort(filter_data_table, decreasing=T)
data_number < -sort(filter_data_table, decreasing=T)
sort(filter_data_table, decreasing=T) %>% data_number
sort(filter_data_table, decreasing=T)
filter_data_table
data_sorted <- sort(filter_data_table, decreasing=T)
data_sorted
sort(filter_data_table, decreasing=T) %>% head(30)
filter_data
filter_data_table
sort(filter_data_table0
sort(filter_data_table)
type(data_unlist)
typeof(data_unlist)
my_new_data = TermDocumentMatrix(data_unlist, control=list(wordLengths=c(2,Inf))
str(my_new_data)
my_new_data = TermDocumentMatrix(data_unlist, control=list(wordLengths=c(2,Inf)))
??TermDocumnetMatrix
typeof(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
data2$description = gsub('\n|\t|<.*?>|&quot;',' ',data2$description)
data2$description = gsub('[^가-힣a-zA-Z]',' ',data2$description)
data2$description = gsub(' +',' ',data2$description)
data2$description = gsub('곱창',' ',data2$description)
data2$description = gsub('으로',' ',data2$description)
data2$description = gsub('진짜',' ',data2$description)
data2$description = gsub('오랜만',' ',data2$description)
data2$description = gsub('근처',' ',data2$description)
data2$description = gsub('이번',' ',data2$description)
data2$description = gsub('얼마',' ',data2$description)
nouns=KoNLP::extractNoun(data2$description)
data_unlist = unlist(nouns)
filter_data = Filter(function(x){nchar(x)>=2}, data_unlist)
filter_data_table = table(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
data2$description = gsub('들이',' ',data2$description)
nouns=KoNLP::extractNoun(data2$description)
data_unlist = unlist(nouns)
filter_data = Filter(function(x){nchar(x)>=2}, data_unlist)
filter_data_table = table(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
data2$description = gsub('때문',' ',data2$description)
nouns=KoNLP::extractNoun(data2$description)
data_unlist = unlist(nouns)
filter_data = Filter(function(x){nchar(x)>=2}, data_unlist)
filter_data_table = table(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
data2$description = gsub('해서',' ',data2$description)
nouns=KoNLP::extractNoun(data2$description)
data_unlist = unlist(nouns)
filter_data = Filter(function(x){nchar(x)>=2}, data_unlist)
filter_data_table = table(filter_data)
sort(filter_data_table, decreasing=T) %>% head(30)
head(nouns)
cnts <- cnts[cnts > 2 & nchar(names(cnts))>1]
cnts <- data_unlist[data_unlist > 2 & nchar(names(data_unlist)>1]
cnts <- data_unlist[data_unlist > 2 & nchar(names(data_unlist))>1]
wordcloud2(data.frame(word=names(cnts_), freq=as.numeric(cnts_)),
color = "random-light", backgroundColor = "black", shape="cloud")
if(!require(wordcloud2)){install.packages('wordcloud') ; library(wordcloud2)} # RColorBrewer()함수 제공
install.packages("wordcloud")
wordcloud2(data.frame(word=names(cnts_), freq=as.numeric(cnts_)),
color = "random-light", backgroundColor = "black", shape="cloud")
if(!require(wordcloud2)){install.packages('wordcloud2') ; library(wordcloud2)} # RColorBrewer()함수 제공
wordcloud2(data.frame(word=names(cnts_), freq=as.numeric(cnts_)),
color = "random-light", backgroundColor = "black", shape="cloud")
wordcloud2(data.frame(word=names(cnts), freq=as.numeric(cnts_)),
color = "random-light", backgroundColor = "black", shape="cloud")
wordcloud2(data.frame(word=names(cnts), freq=as.numeric(cnts)),
color = "random-light", backgroundColor = "black", shape="cloud")
typeof(data2)
head(data2)
View(data2)
data2$description
data2$description[1]
str(data2$description)
typeof(data2$description)
typeof(data2)
?getwd()
getwd(0)
getwd()
setwd(C:\Users\renz\Documents\GitHub\Python\네이버공모전)
sewd('C:/Users/renz/Documents/GitHub/Python/네이버공모전')
setwd('C:/Users/renz/Documents/GitHub/Python/네이버공모전')
# 출처: https://github.com/tirthajyoti/DeepNetworksR/blob/master/wine_classifier.R
# Load test and train set made by 현
train <- read.csv("white_wine_stratified_train.csv", sep=',')
train <-as.data.frame(train)
test <- read.csv("white_wine_stratified_test.csv", sep=',')
test <- as.data.frame(test)
# Scale (normalize) the input data to make neural network work properly
train1<- as.data.frame(scale(train[2:13]))      # 스케일링
test1<-as.data.frame(scale(test[2:13]))         # 범주형 변수 Scale해도되나?
train[2:13]<-train1
test[2:13]<-test1
# Create formula to use in the neural network model from column names of the data set
v<- colnames(test)
m<-v[2]
for (i in 3:length(v)){
m<-paste(m,v[i], sep="+")
}
m<- paste(v[1],'~',m)
f<- as.formula(m)
# Load softmax neural network library and fit model
# 3 hidden layers of 5 neurons each, feel free to change and experiment
# Mini-batch size of 10; activation function = RELU, learning rate = 0.1
# Algorithm = rmsprop, maximum iterations = 500
# Feel free to play with these parameters and train your own model
install.packages('softmaxreg')
library(softmaxreg)
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(5,5,5),
funName = "relu",type='class',
batch=10,rang=1,rate=0.1,threshold = 0.01,maxit=3)
# Scale (normalize) the input data to make neural network work properly
train1<- as.data.frame(scale(train[2:13]))      # 스케일링
test1<-as.data.frame(scale(test[2:13]))         # 범주형 변수 Scale해도되나?
train[2:13]<-train1
test[2:13]<-test1
head(train)
head(train)
# Scale (normalize) the input data to make neural network work properly
train1<- as.data.frame(scale(train[2:13]))      # 스케일링
test1<-as.data.frame(scale(test[2:13]))         # 범주형 변수 Scale해도되나?
train[1:12]<-train1
test[1:12]<-test1
head(train)
# Scale (normalize) the input data to make neural network work properly
train1<- as.data.frame(scale(train[1:12]))      # 스케일링
test1<-as.data.frame(scale(test[1:12]))         # 범주형 변수 Scale해도되나?
train[1:12]<-train1
test[1:12]<-test1
head(train)
# 출처: https://github.com/tirthajyoti/DeepNetworksR/blob/master/wine_classifier.R
# Load test and train set made by 현
train <- read.csv("white_wine_stratified_train.csv", sep=',')
train <-as.data.frame(train)
test <- read.csv("white_wine_stratified_test.csv", sep=',')
test <- as.data.frame(test)
# Scale (normalize) the input data to make neural network work properly
train1<- as.data.frame(scale(train[1:12]))      # 스케일링
test1<-as.data.frame(scale(test[1:12]))         # 범주형 변수 Scale해도되나?
train[1:12]<-train1
test[1:12]<-test1
head(train)
# Create formula to use in the neural network model from column names of the data set
v<- colnames(test)
m<-v[2]
for (i in 3:length(v)){
m<-paste(m,v[i], sep="+")
}
m<- paste(v[1],'~',m)
f<- as.formula(m)
# Load softmax neural network library and fit model
# 3 hidden layers of 5 neurons each, feel free to change and experiment
# Mini-batch size of 10; activation function = RELU, learning rate = 0.1
# Algorithm = rmsprop, maximum iterations = 500
# Feel free to play with these parameters and train your own model
install.packages('softmaxreg')
install.packages("softmaxreg")
library(softmaxreg)
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(5,5,5),
funName = "relu",type='class',
batch=10,rang=1,rate=0.1,threshold = 0.01,maxit=3)
library(softmaxreg)
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(5,5,5),
funName = "relu",type='class',
batch=10,rang=1,rate=0.1,threshold = 0.01,maxit=500)
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(5,5,5),
funName = "relu",type='class',
batch=10,rang=1,rate=0.1,threshold = 0.01,maxit=3)
softmax.model
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[1:12])
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test)
?predict
str(softmax.model)
str(test)
head(test)
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test)
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test, type = 'class')
head(train)
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[2:14])
typeof(test)
class(test)
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[2:12])
test[2:12]
head(test[2:12])
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[1:12])
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[212])
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[2:12])
hist(p,breaks=10,main = "Class of wine predicted by the neural network")
# Create classification error table for 3 classes and print overall accuracy
tab<- as.matrix(table(test$Class,p))
# Scale (normalize) the input data to make neural network work properly
train1<- as.data.frame(scale(train))      # 스케일링
# 출처: https://github.com/tirthajyoti/DeepNetworksR/blob/master/wine_classifier.R
# Load test and train set made by 현
train <- read.csv("white_wine_stratified_train.csv", sep=',')
train <-as.data.frame(train)
test <- read.csv("white_wine_stratified_test.csv", sep=',')
test <- as.data.frame(test)
test1<-as.data.frame(scale(test))         # 범주형 변수 Scale해도되나?
train<-train1
test<-test1
head(train)
# Create formula to use in the neural network model from column names of the data set
v<- colnames(test)
m<-v[2]
for (i in 3:length(v)){
m<-paste(m,v[i], sep="+")
}
m<- paste(v[1],'~',m)
f<- as.formula(m)
# Load softmax neural network library and fit model
# 3 hidden layers of 5 neurons each, feel free to change and experiment
# Mini-batch size of 10; activation function = RELU, learning rate = 0.1
# Algorithm = rmsprop, maximum iterations = 500
# Feel free to play with these parameters and train your own model
install.packages('softmaxreg')
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(5,5,5),
funName = "relu",type='class',
batch=10,rang=1,rate=0.1,threshold = 0.01,maxit=3)
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(5,5,5),
funName = "relu",type='class',
batch=10,rang=1,rate=0.1,threshold = 0.01,maxit=1)
2
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[2:12])
hist(p,breaks=10,main = "Class of wine predicted by the neural network")
# Create classification error table for 3 classes and print overall accuracy
tab<- as.matrix(table(test$Class,p))
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(2,1),
funName = "relu",type='class',
batch=10,rang=1,rate=0.1,threshold = 0.01,maxit=1)
softmax.model<- softmaxReg(formula=f,data=train,hidden=c(2,1),
funName ="relu",type='class',
batch=100 ,rang=1,rate=0.1,threshold = 0.01,maxit=1)
# Predit using the fitted model
# Show histogram of the predicted classes (as factors)
p<-predict(softmax.model,test[2:12])
hist(p,breaks=10,main = "Class of wine predicted by the neural network")
# Create classification error table for 3 classes and print overall accuracy
tab<- as.matrix(table(test$Class,p))
accu=(tab[1,1]+tab[2,2])/(tab[1,1]+tab[1,2]+tab[2,1]+tab[2,2])
white <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', header = TRUE, sep = ';')
red <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', header = TRUE, sep = ';')
white <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', header = TRUE, sep = ';')
library(corrplot)  # graphical display of the correlation matrix
install.packages('corrplot'); library(corrplot)  # graphical display of the correlation matrix
library(caret)     # classification and regression training
install.packages('caret'); library(caret)     # classification and regression training
library(klaR)      # naive bayes
install.packages('nnet')library(nnet)      # neural networks (nnet and avNNet)
install.packages('nnet');library(nnet)      # neural networks (nnet and avNNet)
install.packages("nnet")
install.packages('kernlab')library(kernlab)   # support vector machines (svmLinear and svmRadial)
install.packages('kernlab');library(kernlab)   # support vector machines (svmLinear and svmRadial)
install.packages("kernlab")
install.packages('randomForest');library(randomForest)  # random forest, also for recursive feature elimination
library(gridExtra) # save dataframes as images
install.packages('gridExtra');library(gridExtra) # save dataframes as images
install.packages("gridExtra")
install.packages('doSNOW')
library(doSNOW)    # parallel processing
registerDoSNOW(makeCluster(3, type = 'SOCK'))
red <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', header = TRUE, sep = ';')
white <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', header = TRUE, sep = ';')
good_ones <- df$quality >= 6
bad_ones <- df$quality < 6
good_ones <- df$quality >= 6
red[, 'color'] <- 'red'
white[, 'color'] <- 'white'
df <- rbind(red, white)
df$color <- as.factor(df$color)
good_ones <- df$quality >= 6
bad_ones <- df$quality < 6
df[good_ones, 'quality'] <- 'good'
df[bad_ones, 'quality'] <- 'bad'
df$quality <- as.factor(df$quality)
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
df_dummied[, 'quality'] <- df$quality
dummies <- dummyVars(quality ~ ., data = df)
?dummyVars
??dummyVars
install.packages('caret'); library(caret)     # classification and regression traininglibrary(klaR)      # naive bayes
dummies <- dummyVars(quality ~ ., data = df)
install.packages('kernlab');library(kernlab)   # support vector machines (svmLinear and svmRadial)
install.packages('nnet');library(nnet)      # neural networks (nnet and avNNet)
install.packages('caret'); library(caret)     # classification and regression traininglibrary(klaR)      # naive bayes
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
df_dummied <- data.frame(predict(dummies, newdata = df))
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
df
head(df)
good_ones <- df$quality >= 6
bad_ones <- df$quality < 6
red <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', header = TRUE, sep = ';')
white <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', header = TRUE, sep = ';')
red[, 'color'] <- 'red'
white[, 'color'] <- 'white'
df <- rbind(red, white)
df$color <- as.factor(df$color)
good_ones <- df$quality >= 6
bad_ones <- df$quality < 6
df[good_ones, 'quality'] <- 'good'
df[bad_ones, 'quality'] <- 'bad'
df$quality <- as.factor(df$quality)
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
df_dummied[, 'quality'] <- df$quality
df$quality <- as.factor(df$quality)
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
str(df)
df$quality <- as.factor(df$quality)
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
head(df)
df$color <- as.factor(df$color)
good_ones <- df$quality >= 6
good_ones <- df$quality >= 6
red[, 'color'] <- 'red'
white[, 'color'] <- 'white'
df <- rbind(red, white)
df$color <- as.factor(df$color)
good_ones <- df$quality >= 6
bad_ones <- df$quality < 6
df[good_ones, 'quality'] <- 'good'
df[bad_ones, 'quality'] <- 'bad'
df$quality <- as.factor(df$quality)
head(df)
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
whole <- read.csv('winequality-white.csv', sep= ',')
str(whole)
hist(p,breaks=10,main = "Class of wine predicted by the neural network")
hist(p,breaks=50,main = "Class of wine predicted by the neural network")
hist(p,breaks=50,main = "Class of wine predicted by the neural network")
